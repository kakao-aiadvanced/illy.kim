import streamlit as st
import json
import requests

st.set_page_config(
    page_title="Gemma ì±—ë´‡",
    page_icon="ğŸ’­",
    layout="wide",
    initial_sidebar_state="collapsed",
)

st.title("ğŸ’­ Gemma3 12b")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    temperature = st.slider("Temperature (ì°½ì˜ì„± ì¡°ì ˆ)", 0.0, 2.0, 0.7, 0.1)
    memory_length = st.slider("ë©”ëª¨ë¦¬ ê¸¸ì´ (ì´ì „ ëŒ€í™” ìˆ˜)", 0, 10, 3, 1)
    system_prompt = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
        value="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
        height=100,
    )

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


def create_prompt_with_memory(new_prompt, memory_length):
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    full_prompt = f"{system_prompt}\n\n"

    # ì´ì „ ëŒ€í™” ë‚´ìš© ì¶”ê°€
    if memory_length > 0:
        memory_messages = st.session_state.messages[
            -memory_length * 2 :
        ]  # ê° ëŒ€í™”ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ 2ê°œì˜ ë©”ì‹œì§€ë¥¼ ê°€ì§
        for msg in memory_messages:
            role_prefix = "ì‚¬ìš©ì: " if msg["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸: "
            full_prompt += f"{role_prefix}{msg['content']}\n"

    # ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    full_prompt += f"ì‚¬ìš©ì: {new_prompt}\nì–´ì‹œìŠ¤í„´íŠ¸: "

    return full_prompt


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Ollama API í˜¸ì¶œ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        try:
            # ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            full_prompt = create_prompt_with_memory(prompt, memory_length)

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "gemma3:12b",
                    "prompt": full_prompt,
                    "temperature": temperature,
                    "stream": True,
                },
                stream=True,
            )

            for line in response.iter_lines():
                if line:
                    json_response = json.loads(line)
                    response_part = json_response.get("response", "")
                    full_response += response_part
                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸
                    message_placeholder.markdown(full_response + "â–Œ")

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ (ì»¤ì„œ ì œê±°)
            message_placeholder.markdown(full_response)
            # ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )
        except Exception as e:
            message_placeholder.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.rerun()
